|ID|标题  | 数据集更新日期 | 数据集提供者                           | 许可 | 说明                                                         | 关键字       | 类别         | 论文地址                                      | 备注 |
|---|------------------------------------ | -------------- | -------------------------------------- | ---- | ------------------------------------------------------------ | ------------ | ------------ | --------------------------------------------- | ---- |
|1|[NLPIR微博内容语料库-23万条]([http://www.nlpir.org/wordpress/2017/12/03/nlpir%e5%be%ae%e5%8d%9a%e5%86%85%e5%ae%b9%e8%af%ad%e6%96%99%e5%ba%93-23%e4%b8%87%e6%9d%a1/](http://www.nlpir.org/wordpress/2017/12/03/nlpir微博内容语料库-23万条/))| 2017年12月     | 北京理工大学网络搜索挖掘与安全实验室张华平博士 |      | NLPIR微博内容语料库说明 1.NLPIR微博内容语料库由北京理工大学网络搜索挖掘与安全实验室张华平博士，通过公开采集与抽取从新浪微博、腾讯微博中获得。为了推进微博计算的研究，现通过自然语言处理与信息检索共享平台(127.0.0.1/wordpress)予以公开共享其中的23万条数据（目前已有数据接近1000万，已经剔除了大量的冗余数据）。 2.本语料库在公开过程中，已经最大限度地采用技术手段屏蔽了用户真实姓名和url，如果涉及到的用户需要全面保护个人隐私的，可以Email给张华平博士kevinzhang@bit.edu.cn予以删除，对给您造成的困扰表示抱歉，并希望谅解； 3.只适用于科研教学用途，不得作为商用；引用本语料库，恭请在软件或者论文等成果特定位置表明出处为：NLPIR微博语料库，出处为自然语言处理与信息检索共享平台(http://www.nlpir.org/)。 4.字段说明： id  文章编号 article  正文 discuss  评论数目 insertTime 正文插入时间 origin  来源 person_id 所属人物的id time  正文发布时间 transmit 转发 |        |      |          |      |
|2|[500万微博语料](http://www.nlpir.org/wordpress/download/weibo.7z)| 2018年1月      | 北京理工大学网络搜索挖掘与安全实验室张华平博士 |      | 【500万微博语料】北理工搜索挖掘实验室主任@ICTCLAS张华平博士 提供500万微博语料供大家使用，文件为sql文件，只能导入mysql数据库，内含建表语句，共500万数据。语料只适用于科研教学用途，不得作为商用；引用本语料库，请在软件或者论文等成果特定位置表明出处 。   【看起来这份数据比上面那一份要杂糅一些，没有做过处理】 |        |      |          |      |
|3|[NLPIR新闻语料库-2400万字](http://www.nlpir.org/wordpress/download/NLPIR-news-corpus.rar)| 2017年7月      | [www.NLPIR.org](http://www.nlpir.org/)         |      | NLPIR新闻语料库说明   1.解压缩后数据量为48MB，大约2400万字的新闻； 2.采集的新闻时间跨度为2009年10月12日至2009年12月14日。 3.文件名为新闻的时间；每个文件包括多个新闻正文内容（已经去除了新闻的垃圾信息）； 4.新闻本身内容的版权属于原作者或者新闻机构； 5.整理后的语料库版权属于www.NLPIR.org； 6.可供新闻分析、自然语言处理、搜索等应用提供测试数据场景； 如需更大规模的语料库，可以联系NLPIR.org管理员。 |        |      |          |      |
|4|[NLPIR微博关注关系语料库100万条](http://www.nlpir.org/wordpress/download/weibo_relation_corpus.rar)| 2017年12月     | 北京理工大学网络搜索挖掘与安全实验室张华平博士 |      | NLPIR微博关注关系语料库说明 1.NLPIR微博关注关系语料库由北京理工大学网络搜索挖掘与安全实验室张华平博士，通过公开采集与抽取从新浪微博、腾讯微博中获得。为了推进微博计算的研究，现通过自然语言处理与信息检索共享平台(127.0.0.1/wordpress)予以公开共享其中的1000万条数据（目前已有数据接近10亿，已经剔除了大量的冗余数据）； 2.本语料库在公开过程中，已经最大限度地采用技术手段屏蔽了用户真实姓名和url，如果涉及到的用户需要全面保护个人隐私的，可以Email给张华平博士kevinzhang@bit.edu.cn予以删除，对给您造成的困扰表示抱歉，并希望谅解； 3.只适用于科研教学用途，不得作为商用；引用本语料库，恭请在软件或者论文等成果特定位置表明出处为：NLPIR微博语料库，出处为自然语言处理与信息检索共享平台(http://www.nlpir.org/)。 4.字段说明： person_id  人物的id guanzhu_id 所关注人的id |        |      |          |      |
|5|[NLPIR微博博主语料库100万条]([http://www.nlpir.org/wordpress/2017/09/02/nlpir%e5%be%ae%e5%8d%9a%e5%8d%9a%e4%b8%bb%e8%af%ad%e6%96%99%e5%ba%93100%e4%b8%87%e6%9d%a1/](http://www.nlpir.org/wordpress/2017/09/02/nlpir微博博主语料库100万条/))| 2017年9月      | 北京理工大学网络搜索挖掘与安全实验室张华平博士 |      | NLPIR微博博主语料库说明 1.NLPIR微博博主语料库由北京理工大学网络搜索挖掘与安全实验室张华平博士，通过公开采集与抽取从新浪微博、腾讯微博中获得。为了推进微博计算的研究，现通过自然语言处理与信息检索共享平台(127.0.0.1/wordpress)予以公开共享其中的100万条数据（目前已有数据接近1亿，已经剔除了大量的冗余与机器粉丝） 2.本语料库在公开过程中，已经最大限度地采用技术手段屏蔽了用户真实姓名和url，如果涉及到的用户需要全面保护个人隐私的，可以Email给张华平博士kevinzhang@bit.edu.cn予以删除，对给您造成的困扰表示抱歉，并希望谅解； 3.只适用于科研教学用途，不得作为商用；引用本语料库，恭请在软件或者论文等成果特定位置表明出处为：NLPIR微博语料库，出处为自然语言处理与信息检索共享平台(http://www.nlpir.org/)。 4.字段说明： id  内部id sex  性别 address  家庭住址 fansNum  粉丝数目 summary  个人摘要 wbNum  微博数量 gzNum   关注数量 blog  博客地址 edu  教育情况 work  工作情况 renZh  是否认证 brithday 生日； |        |      |          |      |
|6|[NLPIR短文本语料库-40万字]([http://www.nlpir.org/wordpress/2017/08/12/nlpir%e7%9f%ad%e6%96%87%e6%9c%ac%e8%af%ad%e6%96%99%e5%ba%93-40%e4%b8%87%e5%ad%97/](http://www.nlpir.org/wordpress/2017/08/12/nlpir短文本语料库-40万字/))| 2017年8月      | 北京理工大学网络搜索挖掘与安全实验室 (SMS@BIT) |      | NLPIR短文本语料库说明   1.解压缩后数据量为48万字，大约8704篇短文本内容； 2.整理后的语料库版权属于www.NLPIR.org； 3.可供短文本自然语言处理、搜索、舆情分析等应用提供测试数据场景； |        |      |          |      |
|7|[维基百科语料库](https://dumps.wikimedia.org/zhwiki/)| \              | 维基百科                                       |      | 维基百科会定期打包发布语料库                                 |        |      |          |      |
|8|[古诗词数据库]([https://github.com/chinese-poetry/chinese-poetry](https://link.zhihu.com/?target=https%3A//github.com/chinese-poetry/chinese-poetry))| 2020年         | github主爬虫，http://shici.store               |      |                                                              |        |      |          |      |
|9|[保险行业语料库](https://github.com/chatopera/insuranceqa-corpus-zh)| 2017年         |                                                |      | 该语料库包含从网站Insurance Library 收集的问题和答案。  据我们所知，这是保险领域首个开放的QA语料库：  该语料库的内容由现实世界的用户提出，高质量的答案由具有深度领域知识的专业人士提供。 所以这是一个具有真正价值的语料，而不是玩具。  在上述论文中，语料库用于答复选择任务。 另一方面，这种语料库的其他用法也是可能的。 例如，通过阅读理解答案，观察学习等自主学习，使系统能够最终拿出自己的看不见的问题的答案。  数据集分为两个部分“问答语料”和“问答对语料”。问答语料是从原始英文数据翻译过来，未经其他处理的。问答对语料是基于问答语料，又做了分词和去标去停，添加label。所以，"问答对语料"可以直接对接机器学习任务。如果对于数据格式不满意或者对分词效果不满意，可以直接对"问答语料"使用其他方法进行处理，获得可以用于训练模型的数据。 |        |      |          |      |
|10|[汉语拆字字典](https://github.com/kfcd/chaizi)| 1905年7月      |                                                |      | 本倉庫含開放詞典網用以提供字旁和部件查詢的拆字字典數據庫，有便利使用者查難打漢字等用途。目前數據庫收錄17,803不同漢字的拆法，分為繁體字（chaizi-ft.txt）和簡體字（chaizi-jt.txt）兩個版本。  拆字法有別於固有的筆順字庫。拆字著重於儘量把每個字拆成兩個以上的組成部件，而不是拆成手寫字時所使用的筆畫。 |        |      |          |      |
|11|[新闻预料](https://github.com/brightmart/nlp_chinese_corpus)| 2016年         | 徐亮                                           |      | 可以做为【通用中文语料】，训练【词向量】或做为【预训练】的语料；  也可以用于训练【标题生成】模型，或训练【关键词生成】模型（选关键词内容不同于标题的数据）；  亦可以通过新闻渠道区分出新闻的类型。 |        |      |          |      |
|12|[百科类问答json版(baike2018qa)](https://github.com/brightmart/nlp_chinese_corpus)| 2018年         | 徐亮                                           |      | 可以做为通用中文语料，训练词向量或做为预训练的语料；也可以用于构建百科类问答；其中类别信息比较有用，可以用于做监督训练，从而构建  更好句子表示的模型、句子相似性任务等。 |        |      |          |      |
|13|[社区问答json版(webtext2019zh) ：大规模高质量数据集](https://github.com/brightmart/nlp_chinese_corpus)| 2019年         | 徐亮                                           |      | 1）构建百科类问答：输入一个问题，构建检索系统得到一个回复或生产一个回复；或根据相关关键词从，社区问答库中筛选出你相关的领域数据  2）训练话题预测模型：输入一个问题(和或描述)，预测属于话题。  3）训练社区问答(cQA)系统：针对一问多答的场景，输入一个问题，找到最相关的问题，在这个基础上基于不同答案回复的质量、    问题与答案的相关性，找到最好的答案。  4）做为通用中文语料，做大模型预训练的语料或训练词向量。其中类别信息也比较有用，可以用于做监督训练，从而构建更好句子表示的模型、句子相似性任务等。  5）结合点赞数量这一额外信息，预测回复的受欢迎程度或训练答案评分系统。 |        |      |          |      |
|14|[.维基百科json版(wiki2019zh)](https://github.com/brightmart/nlp_chinese_corpus)| 2019年         | 徐亮                                           |      | 可以做为通用中文语料，做预训练的语料或构建词向量，也可以用于构建知识问答。【不同于wiki原始释放的数据集，这个处理过了】 |        |      |          |      |
